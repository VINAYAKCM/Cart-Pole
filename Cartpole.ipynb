{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b837ef",
   "metadata": {},
   "source": [
    "### 1. IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0305d742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /Applications/anaconda3/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.13 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: cloudpickle in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.1)\n",
      "Requirement already satisfied: pandas in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (3.7.1)\n",
      "Requirement already satisfied: opencv-python in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.8.1.78)\n",
      "Requirement already satisfied: pygame in /Users/vinayakcm/.local/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.5.2)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.65.0)\n",
      "Collecting rich (from stable-baselines3[extra])\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
      "  Using cached Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pillow in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (9.4.0)\n",
      "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
      "  Using cached AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.29.0)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
      "  Using cached AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.3.0 in /Applications/anaconda3/lib/python3.11/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
      "  Downloading ale_py-0.8.1-cp311-cp311-macosx_10_15_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.59.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.23.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.8.0)\n",
      "Requirement already satisfied: six>1.9 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.9.0)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/anaconda3/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2022.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Collecting importlib-resources (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mdurl~=0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Applications/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=0aae213a82d5b430dc3b762d8df20662795336586ff41b1721cbc271d5096e9d\n",
      "  Stored in directory: /Users/vinayakcm/Library/Caches/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: importlib-resources, shimmy, rich, AutoROM.accept-rom-license, autorom, ale-py\n",
      "  Attempting uninstall: shimmy\n",
      "    Found existing installation: Shimmy 2.0.0\n",
      "    Uninstalling Shimmy-2.0.0:\n",
      "      Successfully uninstalled Shimmy-2.0.0\n",
      "  Attempting uninstall: ale-py\n",
      "    Found existing installation: ale-py 0.9.1\n",
      "    Uninstalling ale-py-0.9.1:\n",
      "      Successfully uninstalled ale-py-0.9.1\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 importlib-resources-6.4.0 rich-13.7.1 shimmy-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install 'stable-baselines3[extra]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599e0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 23:53:42.508439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #Average reward, Standard deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab542e",
   "metadata": {},
   "source": [
    "### 2. LOAD ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2fcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7195ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:22.0\n",
      "Episode:2 Score:22.0\n",
      "Episode:3 Score:23.0\n",
      "Episode:4 Score:29.0\n",
      "Episode:5 Score:14.0\n",
      "Episode:6 Score:14.0\n",
      "Episode:7 Score:41.0\n",
      "Episode:8 Score:9.0\n",
      "Episode:9 Score:40.0\n",
      "Episode:10 Score:23.0\n",
      "Episode:11 Score:13.0\n",
      "Episode:12 Score:34.0\n",
      "Episode:13 Score:17.0\n",
      "Episode:14 Score:17.0\n",
      "Episode:15 Score:13.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511b8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinayak/anaconda3/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.15693821,  0.41432214,  0.21812518, -0.06763028], dtype=float32),\n",
       " 0.0,\n",
       " True,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163725ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98754b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f1ef6",
   "metadata": {},
   "source": [
    "### 3. UNDERSTANDING THE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28385b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.         2.\n",
    "#Push left, Push right [0,1]\n",
    "\n",
    "env.action_space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e97ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.             2.             3.          4.\n",
    "#Cart position, Cart velocity, Pole angle, Pole angular velocity\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ace31",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de12763",
   "metadata": {},
   "source": [
    "### 4. TRAIN AN RL AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c6be09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Projects/CartPole/Training/Saved Models'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training log data is stored in log_path\n",
    "log_path = os.path.join(\"/Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Projects/CartPole/Training/Saved Models\")\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0daeb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Creating the dummy vector env only\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO(\"MlpPolicy\", env, verbose = 1, tensorboard_log = log_path)\n",
    "#MLP - multilayer perceptron is used to generate policy\n",
    "#Verbose - Level of data you need shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a6980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Training/Logs/PPO_16\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 7794 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 5400         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083151385 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.686       |\n",
      "|    explained_variance   | 0.00139      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.87         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0179      |\n",
      "|    value_loss           | 56.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5086        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009067893 |\n",
      "|    clip_fraction        | 0.0645      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.0751      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 39.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4943        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008372562 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 54.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4859        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008328095 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 64.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4804        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011173419 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4763        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005142433 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4728         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031497302 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    value_loss           | 53.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4703        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005554907 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    value_loss           | 48.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4684         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064787855 |\n",
      "|    clip_fraction        | 0.0816       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.47         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 24.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x28d824f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Agent\n",
    "model.learn(total_timesteps = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0525785",
   "metadata": {},
   "source": [
    "### 5. SAVE AND RELOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832c71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join(\"/Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Projects/CartPole/Training/Saved Models/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67452a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd08070",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea88aac",
   "metadata": {},
   "source": [
    "# 5. EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc95e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes = 10, render = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103687c2",
   "metadata": {},
   "source": [
    "### 6. TESTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5d5d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: [185.]\n",
      "Episode: 2, Score: [500.]\n",
      "Episode: 3, Score: [344.]\n",
      "Episode: 4, Score: [500.]\n",
      "Episode: 5, Score: [477.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    #Prevoius episode data of cart velocity and reseting game to 0\n",
    "    obs = env.reset()  # Removed [0] indexing\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        #Displaying the env\n",
    "        env.render()\n",
    "        \n",
    "        #model.predict(obs): predicts what action to be taken based on obs and stores 2 output values(action & None)\n",
    "        action, _states = model.predict(obs)\n",
    "        \n",
    "        #env.step(action): takes the action needed and stores the observation in the variables\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #Updates score and episode only get over when score = 500\n",
    "        score += reward\n",
    "        \n",
    "    print(\"Episode: {}, Score: {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb068de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.02316703,  0.18071677,  0.01636625, -0.2800344 ]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{'TimeLimit.truncated': False}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10957f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02407388, -0.0469155 ,  0.01582326, -0.02342252]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d758fe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1), None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48102dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.03352444,  0.35671315, -0.00439783, -0.56456494]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{'TimeLimit.truncated': False}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775325f0",
   "metadata": {},
   "source": [
    "### 7. VIEWING LOGS IN TENSORBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c6ea60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just storing one of the training log PPO path into a variable\n",
    "training_log_path = os.path.join(log_path, '/Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Training/Logs/PPO_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa4dc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Training/Logs/PPO_16'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e516503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Typing \"!\" at the beginning of a cell in jupyter notebook allows us to access terminal and directly type command line prompts to it.\n",
    "#This cell allows us to access the training metrics of the agent using tensorflow\n",
    "!tensorboard --logdir=\"{training_log_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE METRICS TO LOOK FOR:\n",
    "# 1. Average reward\n",
    "# 2. Average episode length\n",
    "\n",
    "# TRAINING STRATEGIES\n",
    "# 1. Training for longer\n",
    "# 2. Hyperparameter tunining\n",
    "# 3. Try different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c744ac5",
   "metadata": {},
   "source": [
    "### APPLYING CALLBACKS TO TRAINING STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aec0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we'll recreate the model with specified reward thresholds, so that the model with stop training \n",
    "# at a certain benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0d6eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69cb3893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the place we want to save the best model which we'll find using call backs\n",
    "save_path = os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433f79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Setting up callbacks\n",
    "\n",
    "\n",
    "#This call back is going to be the one which stops the training when a certain threshold is reached.\n",
    "#Here the threshold is 200/500. \n",
    "\n",
    "stop_callback = StopTrainingOnRewardThreshold( reward_threshold=500, verbose=1 )\n",
    "\n",
    "#This call back is going to be the one which will be triggered after each training round.\n",
    "'''\n",
    "COMMENT:\n",
    "Here, we pass through:\n",
    "    'env': environment\n",
    "    'callback_on_new_best = stop_callback': Everytime there is a new best model, it's gonna run stop_callback and if the stop_callback realises the reward is above the threshold it'll stop the training\n",
    "    'eval_freq': How frequently we wanna call the eval_callback\n",
    "    'best_model': This is going to be the best model we get from the call backs\n",
    "'''\n",
    "eval_callback = EvalCallback(env,\n",
    "                            callback_on_new_best = stop_callback,\n",
    "                            eval_freq = 10000,\n",
    "                            best_model_save_path = save_path,\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e05585be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "#Creating a new PPO Model and assign these call backs\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d3eacaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Training/Logs/PPO_18\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.3        |\n",
      "|    ep_rew_mean          | 27.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009529809 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000422   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.39        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 51.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.6        |\n",
      "|    ep_rew_mean          | 37.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008971078 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0733      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51           |\n",
      "|    ep_rew_mean          | 51           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088764075 |\n",
      "|    clip_fraction        | 0.0866       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.638       |\n",
      "|    explained_variance   | 0.222        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.2         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0207      |\n",
      "|    value_loss           | 55.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=161.40 +/- 46.80\n",
      "Episode length: 161.40 +/- 46.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 161        |\n",
      "|    mean_reward          | 161        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00786373 |\n",
      "|    clip_fraction        | 0.0721     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 24.3       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    value_loss           | 71.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.4     |\n",
      "|    ep_rew_mean     | 66.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 78.9        |\n",
      "|    ep_rew_mean          | 78.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005226054 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 70.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 92.5        |\n",
      "|    ep_rew_mean          | 92.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 326         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007831039 |\n",
      "|    clip_fraction        | 0.0737      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 63.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | 109         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008105253 |\n",
      "|    clip_fraction        | 0.0715      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 55.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 127         |\n",
      "|    ep_rew_mean          | 127         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005859772 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.14        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00837    |\n",
      "|    value_loss           | 36          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040872777 |\n",
      "|    clip_fraction        | 0.0517       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.2         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00937     |\n",
      "|    value_loss           | 73.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x293577210>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval_callback is going to be the callback that checks every 10000 timesteps and save a best_new_model and check if reward is past the threshhold\n",
    "model.learn(total_timesteps = 20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72f6d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically, we use the callback library to stop the training when the agent can play perfectly well to reach\n",
    "# the max high score of 500. We automate it that way, which helps to not overtrain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79713d87",
   "metadata": {},
   "source": [
    "### CHANGING POLICIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7071429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of specifying a different architecture used for in the neural networks used in PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4219e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi = [128, 128, 128, 128], vf = [128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87ff8bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinayak/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/policies.py:484: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path, policy_kwargs = {'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bd42c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /Users/vinayak/Documents/Machine Learning Projects/Reinforcement Learning/Training/Logs/PPO_19\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | 20.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.3        |\n",
      "|    ep_rew_mean          | 27.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014722402 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.00164    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.15        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.4        |\n",
      "|    ep_rew_mean          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017495954 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.66        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 22          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 51.7       |\n",
      "|    ep_rew_mean          | 51.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 44         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 182        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01357661 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.598     |\n",
      "|    explained_variance   | 0.469      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18         |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    value_loss           | 39.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=203.20 +/- 44.74\n",
      "Episode length: 203.20 +/- 44.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 203         |\n",
      "|    mean_reward          | 203         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012704723 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.6     |\n",
      "|    ep_rew_mean     | 65.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 83.5        |\n",
      "|    ep_rew_mean          | 83.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011947654 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009891974 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.06        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 117         |\n",
      "|    ep_rew_mean          | 117         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 384         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009308011 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.3         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 135         |\n",
      "|    ep_rew_mean          | 135         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010831773 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 6.54        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=473.80 +/- 52.40\n",
      "Episode length: 473.80 +/- 52.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 474          |\n",
      "|    mean_reward          | 474          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068393387 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.512       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.395        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 5.82         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x29354db90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653aa39",
   "metadata": {},
   "source": [
    "### USING AN ALTERNATE ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f9ddd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rather than using a PPO model algorithm, we could use a DQN model algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34d839cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f9a4987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = 20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
